# 爬虫分项目介绍

[toc]

## 项目简介

​	项目开启于2020年1月23日，从肥科回到家后一天。丁香园制作了一个[疫情信息网站](https://3g.dxy.cn/newh5/view/pneumonia)，可以查看确诊人数，疑似病例，治愈人数，死亡人数。于是决定定时爬取网站上的人数信息(全国/各省区)。一开始的目的仅仅是为了练习一下**爬虫**，后来疫情信息越来越严重就感觉这些信息可以记录一下这段历史，或许会有用呢？（大雾）:sweat_smile:。我设定1个小时爬取一次信息，不过由于丁香园的web一直在迭代经常半夜就报错，所以爬取的信息是有遗漏的。:black_heart:

​	version2.0增加了世界各国的详细信息及全球信息汇总（除中国）。2月16晚上感觉这次疫情全球范围可能也不容乐观，所以增添了这些信息的爬取。

----

## 各文件简介

1. Config.py：配置文件
2. URLDealer.py：处理http请求获得response，或者用bs4包中BeautifulSoup封装response
3. Plague_info.py ：解析并提取页面数据
4. MySQL_service.py：数据库链接类，简单的封装
5. Info_saver.py：存储类，将全国各省区信息，全国汇总信息；世界各国信息，各国汇总信息存入数据库
6. select_data.py：提供一个可扩展的查询数据类
7. main.py：本项目的函数入口
8. test.py：本项目的单元测试模块

----

## 各文件详细介绍

#### 	Config.py

​		进行数据库配置；URL请求地址，timeout及请求头；爬取间隔

#### 	URLDealer.py： `URLDealer`类

  1. `get_response()`方法：通过死循环保证http请求成功时(status_code = 200)时得到response对象，在遇到异常或者请求错误时仅打印错误信息。这样就避免程序在面临复杂网络环境时抛出非必要异常。

  2. `get_soup()`方法：将response对象封装为bs4包中的`BeautifulSoup`实例

     ​	**个人感觉这个类在爬虫项目中进行get请求时可以直接复用**

#### Plague_info.py : `PlagueInfo`类

1. 这个类其实很鸡肋 因为各种页面都是不一样的 包括这个项目爬取的丁香园疫情网页，从23号开始爬取到现在我已经修改了5.6次了
2. `china_info()`方法：提取各个省区的信息：确诊人数，疑似人数，治愈人数，死亡人数。先在源数据网页中找到数据存放的tag，将数据以`str`格式取出。剩下的就是使用正则表达式将每个省区的信息提取，放入字典中（字典结果与数据库表格基本对应）。再将每个省的数据放入字典中，以省区名字为键。
3. `world_info()`方法：提取世界各国的信息：提前的信息，数据结构均与`china_info()`方法中一致。提取信息的方法也类似，只是根据文本信息修改了正则表达式。
4. `total_info(detailed_info)`类方法：将各省区信息（各国信息）汇总成全国信息（全世界信息）。

#### MySQL_service.py：`MySQLCurSor`类

1. `MySQLSaver`类：对`pymysql`中数据库连接类简单封装，可以执行查询与增删改，直接写入SQL语句。在本项目中为了将全国信息及各省市的信息存入数据库中。
2. `execute_idu(self, sql, args=None)`方法进行了回滚设置，在数据库增删改出现错误时回滚，并报错。🙄

#### Info_saver.py：`MySQLSaver`类

​	这个类用于将数据存入数据库。由于全国信息与世界信息的数据结构一致，在数据库中存放的表格高度一致，所以此类中方法对称。由于是与数据库交互，所以大量方法设置为私有的辅助方法。

1. `__get_count(self, table_name)`方法：从存储详细信息的库中得到上次存储的序号，用于标记当前存储的序号，参数`table_name`指定读取中国各省区信息还是世界各国信息。
2. `__save_detail_info(self, table_name, detailed_info)`方法：单次存储各省区信息（或各国信息）进数据库。
3. `__save_total_info(self, table_name, total_info)`方法：单次存储全国汇总信息（或世界信息）进数据库。
4. `save_info_once(self)`方法：单次将全国各省区信息，全国汇总信息，世界各国信息，世界汇总信息存储入数据库。

#### select_data.py

1. 构造函数传入要查询的表，及要查询的数据：column_index: 可变参数 可写 1 2 3 4。至少选择一列，至多选择4列，参数按从小到大的顺序

2. `get_base_sql(self)`函数：构造得到sql语句

3. `get_data(self, sql)`函数：执行sql语句，得到查询结果并存放在`self.data`中

    **此基础类可以扩展数据的进一步处理，也可通过继承创建查询指定数据的类**

#### main.py

​	程序**入口**：通过死循环及`time.sleep()`阻塞主进程以按设定间隔运行程序。每次先从web爬去信息，然后创建一个数据库存储实例，完成当前次爬取数据的存储。👏

#### test.py

​	项目单元测试模块。`@test_decorator`装饰器提示测试开始，测试结束及测试结果等信息。在这个文件中可以对各个方法进行单元测试，方便写程序时一个接口一个接口的排除bug

****

## 注意事项

* 数据库的设计及介绍见 [SQL_mysql](https://github.com/ustcyyw/wuhan_plague/tree/master/version1.1/SQL_mysql) 中，必须开启数据库服务，配置好数据库，建好数据表才能运行本项目，否则数据库异常。
* 丁香园的网页进行改版后可能需要更新 Plague_info.py 不要改变返回值的形式及类型，这样可以保证别的文件完全不需要更改

    









​	

